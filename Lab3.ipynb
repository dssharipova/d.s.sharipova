{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .config(conf=conf)\n",
    "         .appName(\"test\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer, StopWordsRemover \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf, col, isnan, isnull, broadcast, desc, lower, countDistinct, array_contains, array, count, avg, explode, split, lit, when\n",
    "from pyspark.sql.types import FloatType, ArrayType, StringType \n",
    "import json \n",
    "import re\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.ml.linalg import SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "items = spark.read.csv(\"/labs/slaba03/laba03_items.csv\", header=True, inferSchema=True,\n",
    "                          sep='\\t', ignoreLeadingWhiteSpace=True).select(['item_id', 'content_type',\n",
    " 'title',\n",
    " 'year',\n",
    " 'genres'])\n",
    "train = spark.read.csv(\"/labs/slaba03/laba03_train.csv\", header=True, inferSchema=True)\n",
    "test = spark.read.csv(\"/labs/slaba03/laba03_test.csv\", header=True, inferSchema=True)\n",
    "views = spark.read.csv(\"/labs/slaba03/laba03_views_programmes.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Расчет признаков для телепередач\n",
    "views = views.withColumn(\"duration\", col(\"ts_end\") - col(\"ts_start\"))\n",
    "\n",
    "# Время просмотра каждой передачи всеми пользователями\n",
    "item_total_view_time = views.groupBy('item_id').agg(f.sum('duration').alias('item_total_view_time'))\n",
    "\n",
    "# Количество уникальных пользователей, посмотревших передачу\n",
    "item_unique_viewers = views.groupBy('item_id').agg(countDistinct('user_id').alias('item_unique_viewers'))\n",
    "\n",
    "# Объединение признаков для передач\n",
    "item_features = item_total_view_time.join(item_unique_viewers, on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчет признаков для пользователей\n",
    "# Общее время просмотра передач пользователем\n",
    "user_total_view_time = views.groupBy('user_id').agg(f.sum('duration').alias('user_total_view_time'))\n",
    "\n",
    "# Количество передач, просмотренных пользователем\n",
    "user_total_watched_items = views.groupBy('user_id').agg(countDistinct('item_id').alias('user_total_watched_items'))\n",
    "\n",
    "# Объединение признаков для пользователей\n",
    "user_view_features = user_total_view_time.join(user_total_watched_items, on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Доля покупок определенной передачи по train \n",
    "item_total_purchases = train.groupBy('item_id').agg(f.sum('purchase').alias('item_total_purchases'))\n",
    "\n",
    "item_total_lines = train.groupBy('item_id').agg(count('purchase').alias('item_total_lines'))\n",
    "\n",
    "# Соединение результатов\n",
    "item_features = item_total_purchases.join(item_total_lines, on='item_id')\n",
    "\n",
    "# Расчет доли покупок пользователя\n",
    "item_features = item_features.withColumn('item_purchase_ratio', col('item_total_purchases') / col('item_total_lines'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подсчет общего количества покупок пользователем\n",
    "\n",
    "user_total_purchases = train.groupBy('user_id').agg(f.sum('purchase').alias('user_total_purchases'))\n",
    "\n",
    "user_total_lines = train.groupBy('user_id').agg(count('purchase').alias('user_total_lines'))\n",
    "\n",
    "# Соединение результатов\n",
    "user_features = user_total_purchases.join(user_total_lines, on='user_id')\n",
    "\n",
    "# Расчет доли покупок пользователя\n",
    "user_features = user_features.withColumn('user_purchase_ratio', col('user_total_purchases') / col('user_total_lines'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Присоединение признаков пользователей к тренировочному датасету\n",
    "train = train.join(user_view_features, on='user_id', how='left')\n",
    "train = train.join(user_features, on='user_id', how='left')\n",
    "\n",
    "# Присоединение признаков передач к тренировочному датасету\n",
    "train = train.join(item_features, on='item_id', how='left')\n",
    "train = train.join(item_total_view_time, on='item_id', how='left')\n",
    "train = train.join(item_unique_viewers, on='item_id', how='left')\n",
    "\n",
    "# Заполнение пропусков\n",
    "train = train.na.fill(0)\n",
    "\n",
    "# Проверка\n",
    "#train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Выбор признаков и целевой переменной\n",
    "feature_columns = [\n",
    "    'user_total_view_time', 'user_total_watched_items', 'user_purchase_ratio',\n",
    "    'item_total_view_time', 'item_unique_viewers', 'item_purchase_ratio'\n",
    "]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "train = assembler.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train.limit(60000).randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Создание поднаборов данных по 15 000 строк каждый\n",
    "train_df_train = train.sample(False, 50000 / train.count(), seed=42)\n",
    "remaining_data = train.subtract(train_df_train)\n",
    "train_df_test = remaining_data.sample(False, 10000 / remaining_data.count(), seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "gbt = GBTClassifier(labelCol='purchase', featuresCol='features', maxIter=10)\n",
    "model = gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.9396469206313851\n"
     ]
    }
   ],
   "source": [
    "# Предсказание на тренировочных данных\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Оценка модели\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='purchase', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(f'ROC AUC: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка тестовых данных\n",
    "test = test.join(user_total_view_time, on='user_id', how='left')\n",
    "test = test.join(user_total_watched_items, on='user_id', how='left')\n",
    "test = test.join(user_features, on='user_id', how='left')\n",
    "test = test.join(item_total_view_time, on='item_id', how='left')\n",
    "test = test.join(item_unique_viewers, on='item_id', how='left')\n",
    "test = test.join(item_features, on='item_id', how='left')\n",
    "\n",
    "test = test.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание\n",
    "test_predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+--------------------+------------------------+--------------------+----------------+--------------------+--------------------+-------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|item_id|user_id|purchase|user_total_view_time|user_total_watched_items|user_total_purchases|user_total_lines| user_purchase_ratio|item_total_view_time|item_unique_viewers|item_total_purchases|item_total_lines| item_purchase_ratio|            features|       rawPrediction|         probability|prediction|\n",
      "+-------+-------+--------+--------------------+------------------------+--------------------+----------------+--------------------+--------------------+-------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|   8389| 566758|    null|              447954|                     102|                   1|            2655|3.766478342749529E-4|                   0|                  0|                   8|            1338|0.005979073243647235|[447954.0,102.0,3...|[1.32480983238348...|[0.93398754488215...|       0.0|\n",
      "|   8389| 816426|    null|             3162072|                     873|                   0|            2603|                 0.0|                   0|                  0|                   8|            1338|0.005979073243647235|[3162072.0,873.0,...|[1.32641491749104...|[0.93418519179343...|       0.0|\n",
      "|   8389| 892290|    null|             1559239|                     454|                   2|            2591|7.719027402547279E-4|                   0|                  0|                   8|            1338|0.005979073243647235|[1559239.0,454.0,...|[1.32567844371366...|[0.93409457229832...|       0.0|\n",
      "|   8389| 901323|    null|             1335583|                     308|                   1|            2600|3.846153846153846E-4|                   0|                  0|                   8|            1338|0.005979073243647235|[1335583.0,308.0,...|[1.32641491749104...|[0.93418519179343...|       0.0|\n",
      "|   8389| 928231|    null|              217705|                      46|                   2|            2637|7.584376185058779E-4|                   0|                  0|                   8|            1338|0.005979073243647235|[217705.0,46.0,7....|[1.32407335860611...|[0.93389667250630...|       0.0|\n",
      "+-------+-------+--------+--------------------+------------------------+--------------------+----------------+--------------------+--------------------+-------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import numpy as np\n",
    "\n",
    "# Функция для извлечения вероятности положительного класса из DenseVector\n",
    "def extract_probability(probability):\n",
    "    return float(probability[1])\n",
    "\n",
    "# Регистрация UDF\n",
    "extract_probability_udf = udf(extract_probability, DoubleType())\n",
    "\n",
    "# Применение UDF к колонке probability\n",
    "result = test_predictions.withColumn('purchase', extract_probability_udf(col('probability')))\n",
    "\n",
    "# Выбор необходимых колонок и сортировка\n",
    "result = result.select('user_id', 'item_id', 'purchase').orderBy(['user_id', 'item_id'])\n",
    "\n",
    "# Сохранение результатов\n",
    "result.toPandas().to_csv('lab03.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
